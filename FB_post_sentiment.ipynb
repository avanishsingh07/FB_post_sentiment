{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf82fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os, re, csv, math, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import MaxPool1D, Flatten, Conv1D, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a52d44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nFair use rationale for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair use template, you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use.\\n\\nPlease go to the image description page and edit it to include a fair use rationale.\\n\\nIf you have uploaded other fair use media, consider checking that you have specified the fair use rationale on those pages too. You can find a list of \\'image\\' pages you have edited by clicking on the \"\"my contributions\"\" link (it is located at the very top of any Wikipedia page when you are logged in), and then selecting \"\"Image\"\" from the dropdown box. Note that any fair use images uploaded after 4 May, 2006, and lacking such an explanation will be deleted one week after they have been uploaded, as described on criteria for speedy deletion. If you have any questions please ask them at the Media copyright questions page. Thank you. (talk â€¢ contribs â€¢ ) \\nUnspecified source for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I noticed that the file\\'s description page currently doesn\\'t specify who created the content, so the copyright status is unclear. If you did not create this file yourself, then you will need to specify the owner of the copyright. If you obtained it from a website, then a link to the website from which it was taken, together with a restatement of that website\\'s terms of use of its content, is usually sufficient information. However, if the copyright holder is different from the website\\'s publisher, then their copyright should also be acknowledged.\\n\\nAs well as adding the source, please add a proper copyright licensing tag if the file doesn\\'t have one already. If you created/took the picture, audio, or video then the  tag can be used to release it under the GFDL. If you believe the media meets the criteria at Wikipedia:Fair use, use a tag such as  or one of the other tags listed at Wikipedia:Image copyright tags#Fair use. See Wikipedia:Image copyright tags for the full list of copyright tags that you can use.\\n\\nIf you have uploaded other files, consider checking that you have specified their source and tagged them, too. You can find a list of files you have uploaded by following [ this link]. Unsourced and untagged images may be deleted one week after they have been tagged, as described on criteria for speedy deletion. If the image is copyrighted under a non-free license (per Wikipedia:Fair use) then the image will be deleted 48 hours after . If you have any questions please ask them at the Media copyright questions page. Thank you. (talk â€¢ contribs â€¢ ) \"'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "053c3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805966a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Yet the dog had grown old and less capable , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>She disliked the intimacy he showed towards so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral   There are tons of other paintings that I thin...\n",
       "1  sadness  Yet the dog had grown old and less capable , a...\n",
       "2     fear  When I get into the tube or the train without ...\n",
       "3     fear  This last may be a source of considerable disq...\n",
       "4    anger  She disliked the intimacy he showed towards so..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1 = 'https://raw.githubusercontent.com/lukasgarbas/nlp-text-emotion/master/data/data_test.csv'\n",
    "url2='https://raw.githubusercontent.com/lukasgarbas/nlp-text-emotion/master/data/data_train.csv'\n",
    "train = pd.read_csv(url2, encoding='utf-8')\n",
    "test= pd.read_csv(url1, encoding='utf-8')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2218753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I experienced this emotion when my grandfather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>when I first moved in , I walked everywhere ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>` Oh ! \" she bleated , her voice high and rath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>However , does the right hon. Gentleman recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My boyfriend didn't turn up after promising th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  sadness  I experienced this emotion when my grandfather...\n",
       "1  neutral   when I first moved in , I walked everywhere ....\n",
       "2    anger  ` Oh ! \" she bleated , her voice high and rath...\n",
       "3     fear  However , does the right hon. Gentleman recogn...\n",
       "4  sadness  My boyfriend didn't turn up after promising th..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b856d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 500\n",
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1f0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.Text.tolist()\n",
    "X_test = test.Text.tolist()\n",
    "\n",
    "y_train = train.Emotion\n",
    "y_test = test.Emotion\n",
    "\n",
    "data = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32a29b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        2326\n",
      "sadness    2317\n",
      "anger      2259\n",
      "neutral    2254\n",
      "fear       2171\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.Emotion.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ffd7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \n",
    "    # remove hashtags and @usernames\n",
    "    data = re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data = re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "    \n",
    "    # tekenization using nltk\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cee51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(clean_text(text)) for text in data.Text]\n",
    "\n",
    "texts_train = [' '.join(clean_text(text)) for text in X_train]\n",
    "texts_test = [' '.join(clean_text(text)) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f9eb1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bit ? I 'm extremely annoyed that he did n't phone me when he promised me that he would ! He 's such a liar .\n"
     ]
    }
   ],
   "source": [
    "print(texts_train[92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2b94c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 12087\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6a05335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   119,    51,   345],\n",
       "       [    0,     0,     0, ...,    37,   277,   154],\n",
       "       [    0,     0,     0, ...,    16,     2,  1210],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   876,     4,   909],\n",
       "       [    0,     0,     0, ...,     1,     6,   117],\n",
       "       [    0,     0,     0, ..., 10258,   173,    13]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
    "\n",
    "X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ac78430",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'joy': 0,\n",
    "    'fear': 1,\n",
    "    'anger': 2,\n",
    "    'sadness': 3,\n",
    "    'neutral': 4\n",
    "}\n",
    "y_train = [encoding[x] for x in train.Emotion]\n",
    "y_test = [encoding[x] for x in test.Emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5a8491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a89861d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db7891f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word vectors...\n",
      "Unzipping...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    print('Downloading word vectors...')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
    "                              'wiki-news-300d-1M.vec.zip')\n",
    "    print('Unzipping...')\n",
    "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('embeddings')\n",
    "    print('done.')\n",
    "    \n",
    "    os.remove('wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca98fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12088, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
    "embedd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913efa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in wiki vocab: 11442\n",
      "New words found: 645\n"
     ]
    }
   ],
   "source": [
    "new_words = 0\n",
    "for word in index_of_words:\n",
    "    entry = embedd_matrix[index_of_words[word]]\n",
    "    if all(v == 0 for v in entry):\n",
    "        new_words = new_words + 1\n",
    "\n",
    "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
    "print('New words found: ' + str(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "758d084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_layer = Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = max_seq_len,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b179edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "filters = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b3c4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 300)          3626400   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 498, 256)          230656    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,924,133\n",
      "Trainable params: 297,733\n",
      "Non-trainable params: 3,626,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d44aa0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000018255CD15E0>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000018255CD1A60>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(7934,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000018255CD1790>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(7934, 500) dtype=int32>, <tf.Tensor 'args_2:0' shape=(7934, 5) dtype=float32>))\n",
      "    kwargs: {}\n",
      "\n",
      "Epoch 1/6\n",
      "31/31 [==============================] - 16s 520ms/step - loss: 0.5922 - accuracy: 0.7969 - val_loss: 0.7272 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 2/6\n",
      "31/31 [==============================] - 17s 548ms/step - loss: 0.4780 - accuracy: 0.8479 - val_loss: 0.7181 - val_accuracy: 0.7501 - lr: 0.0010\n",
      "Epoch 3/6\n",
      "31/31 [==============================] - 16s 528ms/step - loss: 0.3785 - accuracy: 0.8911 - val_loss: 0.7114 - val_accuracy: 0.7518 - lr: 0.0010\n",
      "Epoch 4/6\n",
      "31/31 [==============================] - 16s 533ms/step - loss: 0.2819 - accuracy: 0.9256 - val_loss: 0.7007 - val_accuracy: 0.7604 - lr: 0.0010\n",
      "Epoch 5/6\n",
      "31/31 [==============================] - 19s 631ms/step - loss: 0.2114 - accuracy: 0.9556 - val_loss: 0.7049 - val_accuracy: 0.7633 - lr: 0.0010\n",
      "Epoch 6/6\n",
      "31/31 [==============================] - 20s 656ms/step - loss: 0.1479 - accuracy: 0.9729 - val_loss: 0.7268 - val_accuracy: 0.7648 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 6\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00000001)\n",
    "\n",
    "callbacks = [learning_rate_reduction, \n",
    "             EarlyStopping('val_loss', patience=3), \n",
    "             ModelCheckpoint('comment_sentiment_type_model.h5', save_best_only=True)]\n",
    "\n",
    "hist = model.fit(X_train_pad, y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_test_pad,y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4192ee49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000018255CD1670>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000018255CD13A0>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(3393,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000018255D07160>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, <tf.Tensor 'args_1:0' shape=(3393, 500) dtype=int32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_predict_function.<locals>.predict_function at 0x0000018255D074C0>\n",
      "    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000018255D1A820>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:<function Model.make_predict_function.<locals>.predict_function at 0x0000018255D074C0> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Converted call: <function Model.make_predict_function.<locals>.step_function at 0x0000018255D07790>\n",
      "    args: (<keras.engine.sequential.Sequential object at 0x00000182555DB370>, <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000018255D1A820>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_predict_function.<locals>.step_function.<locals>.run_step at 0x0000018255C0ED30>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 500) dtype=int32>,)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_pad)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions = [class_names[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3111fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.48%\n",
      "\n",
      "F1 Score: 76.48\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test.Emotion, predictions) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test.Emotion, predictions, average='micro') * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73a9e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000018263F96160>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000018255D50430>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(1,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x000001826BC8BC10>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, <tf.Tensor 'args_1:0' shape=(1, 500) dtype=int32>)\n",
      "    kwargs: {}\n",
      "\n",
      "Message: ['I love you']\n",
      "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
      "predicted: neutral (0.06 seconds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "message = ['''I love you''']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
    "\n",
    "start_time = time.time()\n",
    "pred = model.predict(padded)\n",
    "\n",
    "print('Message: ' + str(message))\n",
    "print('ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ')\n",
    "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934847b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8c6506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weâ€™re proud to continue our collaboration with MyGov to offer Digilocker services on WhatsApp for ef\n",
      "At Meta, we work hard to ensure our users are safe online. One of the ways they can do so is to let \n"
     ]
    }
   ],
   "source": [
    "listposts = []\n",
    "for post in get_posts(\"MetaIndia\", pages=1):#366190054572553\n",
    "    print(post['text'][:100])\n",
    "    listposts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc5c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FB_post_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FB_post_sentiment.py\n",
    "import streamlit as st\n",
    "import filetype\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sys, os, re, csv, math, codecs, numpy as np, pandas as pd\n",
    "import time\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import MaxPool1D, Flatten, Conv1D, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from facebook_scraper import get_posts\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('comment_sentiment_type_model.h5')\n",
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']\n",
    "count=10\n",
    "input_ = st.text_input('Input username', '#India')\n",
    "st.write('Your input', input_)\n",
    "if st.button('Predict_sentiment'):\n",
    "    listposts = []\n",
    "    for post in get_posts(\"MetaIndia\", pages=1):#366190054572553\n",
    "        txt=post['text'][:100]\n",
    "        listposts.append(post)\n",
    "        seq = tokenizer.texts_to_sequences(txt)\n",
    "        padded = pad_sequences(seq, maxlen=500)\n",
    "\n",
    "        start_time = time.time()\n",
    "        pred = model.predict(padded)\n",
    "        st.write('Message: ' + str(txt))\n",
    "        st.write('ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ¤–ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ')\n",
    "        st.write('Predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa94aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter MetaIndia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bae1531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run FB_post_sentiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728d77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
